{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3: Intermediate Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook, we conclude our end-to-end data pipeline through a **predictive** lens: we apply classical machine learning models and algorithms to assess predictive accuracies for our training and testing data. \n",
    "\n",
    "- **NOTE**: Before working through this notebook, please ensure that you have all necessary dependencies as denoted in [Section A: Imports and Initializations](#section-A) of this notebook.\n",
    "\n",
    "- **NOTE**: Before working through Sections A-D of this notebook, please run all code cells in [Appendix A: Supplementary Custom Objects](#appendix-A) to ensure that all relevant functions and objects are appropriately instantiated and ready for use.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”µ TABLE OF CONTENTS ðŸ”µ <a name=\"TOC\"></a>\n",
    "\n",
    "Use this **table of contents** to navigate the various sections of the processing notebook.\n",
    "\n",
    "#### 1. [Section A: Imports and Initializations](#section-A)\n",
    "\n",
    "    All necessary imports and object instantiations for data predictions.\n",
    "\n",
    "#### 2. [Section B: Loading our Processed Data](#section-B)\n",
    "\n",
    "    Loading processed data states for current access.\n",
    "\n",
    "#### 3. [Section C: Machine Learning](#section-C)\n",
    "\n",
    "    Applying classical machine learning algorithms on processed datasets.\n",
    "    \n",
    "#### 4. [Section D: Deep Learning](#section-D)\n",
    "\n",
    "    Applying advanced machine learning and deep learning algorithms on processed datasets. \n",
    "\n",
    "#### 5. [Appendix A: Supplementary Custom Objects](#appendix-A)\n",
    "\n",
    "    Custom Python object architectures used throughout the data processing.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Section A: Imports and Initializations <a name=\"section-A\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Importations for Data Manipulation and Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules for Data Preparation and Model Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms for Data Resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aakashsudhakar/anaconda3/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms for Classical Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Algorithmic Support Structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../structures/\")\n",
    "from custom_structures import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Section B: Loading our Processed Data <a name=\"section-B\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "REL_PATH_PROC_DATA = \"../data/processed/\"\n",
    "DATA_X, DATA_y = \"X/\", \"y/\"\n",
    "SUBDIR_PROC, SUBDIR_SCA, SUBDIR_RED = \"processed/\", \"scaled/\", \"reduced/\"\n",
    "\n",
    "X_TRAIN_PROC, X_TEST_PROC = \"train_pXp.csv\", \"test_pXp.csv\"\n",
    "X_TRAIN_SCA, X_TEST_SCA = \"train_pXs.csv\", \"test_pXs.csv\"\n",
    "X_TRAIN_RED, X_TEST_RED = \"train_pXr.csv\", \"test_pXr.csv\"\n",
    "y_TRAIN_PROC, y_TEST_PROC = \"train_pyp.csv\", \"test_pyp.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data: _Fully Processed X-Datasets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pro = pd.read_csv(REL_PATH_PROC_DATA + DATA_X + SUBDIR_PROC + X_TRAIN_PROC, index_col=0)\n",
    "X_test_pro = pd.read_csv(REL_PATH_PROC_DATA + DATA_X + SUBDIR_PROC + X_TEST_PROC, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data: _Scaled X-Datasets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sca = pd.read_csv(REL_PATH_PROC_DATA + DATA_X + SUBDIR_SCA + X_TRAIN_SCA, index_col=0)\n",
    "X_test_sca = pd.read_csv(REL_PATH_PROC_DATA + DATA_X + SUBDIR_SCA + X_TEST_SCA, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data: _Dimensionally Reduced X-Datasets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_red = pd.read_csv(REL_PATH_PROC_DATA + DATA_X + SUBDIR_RED + X_TRAIN_RED, index_col=0)\n",
    "X_test_red = pd.read_csv(REL_PATH_PROC_DATA + DATA_X + SUBDIR_RED + X_TEST_RED, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data: _Fully Processed Targets (y)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pro = np.ravel(pd.read_csv(REL_PATH_PROC_DATA + DATA_y + SUBDIR_PROC + y_TRAIN_PROC, index_col=0, header=None))\n",
    "y_test_pro = np.ravel(pd.read_csv(REL_PATH_PROC_DATA + DATA_y + SUBDIR_PROC + y_TEST_PROC, index_col=0, header=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Section C: Machine Learning <a name=\"section-C\"></a>\n",
    "\n",
    "#### Models to Use:\n",
    "- **k-Nearest Neighbors Classifier**\n",
    "    - _Hyperparameters_: `n_neighbors`, ~~`leaf_size`~~, `weights`, ~~`algorithm`~~\n",
    "- **Support Vector Classifier**\n",
    "    - _Hyperparameters_: `kernel`, `C`, ~~`gamma`~~, ~~`degree`~~\n",
    "- **Decision Tree Classifier**\n",
    "    - _Hyperparameters_: ~~`max_features`~~, `min_samples_split`, `min_samples_leaf`\n",
    "- **Random Forest Classifier**\n",
    "    - _Hyperparameters_: ~~`criterion`~~, ~~`n_estimators`~~, `min_samples_split`, `min_samples_leaf`\n",
    "- **Logistic Regression Classifier**\n",
    "    - _Hyperparameters_: `penalty`, `C`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine Optimal Hyperparameters for Conducting Cross-Validation-Driven Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hyperparams = {\n",
    "    \"kNN\": (KNeighborsClassifier, {\n",
    "        \"n_neighbors\": [1, 3, 5],\n",
    "#         \"leaf_size\": [1, 2, 3, 5],\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "#         \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
    "    }), \n",
    "    \"svc\": (SVC, {\n",
    "        \"kernel\": [\"linear\"],\n",
    "        \"C\": [0.1, 1, 10, 100],\n",
    "#         \"gamma\": [0.1, 1, 10],\n",
    "#         \"degree\": [0, 1, 2, 3, 4, 5, 6]\n",
    "    }), \n",
    "    \"dtree\": (DecisionTreeClassifier, {\n",
    "#         \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "        \"min_samples_split\": [2, 3, 4, 5, 6],\n",
    "        \"min_samples_leaf\": [1, 2, 3, 4, 5]  \n",
    "    }), \n",
    "    \"rforest\": (RandomForestClassifier, {\n",
    "#         \"criterion\": [\"gini\", \"entropy\"],\n",
    "#         \"n_estimators\": [10, 15, 20, 25, 30],\n",
    "        \"min_samples_split\": [2, 3, 4, 5, 6],\n",
    "        \"min_samples_leaf\": [1, 2, 3, 4, 5]\n",
    "    }), \n",
    "    \"logreg\": (LogisticRegression, {\n",
    "        \"penalty\": [\"l1\", \"l2\"],\n",
    "        \"C\": [0.1, 1, 10]\n",
    "    })\n",
    "}\n",
    "\n",
    "param_table = {\n",
    "    \"processed\": (\n",
    "        (X_train_pro, X_test_pro),\n",
    "        dict.fromkeys(model_hyperparams)\n",
    "    ),\n",
    "    \"scaled\": (\n",
    "        (X_train_sca, X_test_sca),\n",
    "        dict.fromkeys(model_hyperparams)\n",
    "    ),\n",
    "    \"reduced\": (\n",
    "        (X_train_red, X_test_red),\n",
    "        dict.fromkeys(model_hyperparams)\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING ON DATASET: SCALED\n",
      "\tFITTING MODEL: KNN\n",
      "\t\tMODEL FITTED SUCCESSFULLY.\n",
      "\tFITTING MODEL: DTREE\n",
      "\t\tMODEL FITTED SUCCESSFULLY.\n",
      "\tFITTING MODEL: RFOREST\n",
      "\t\tMODEL FITTED SUCCESSFULLY.\n",
      "\tFITTING MODEL: SVC\n",
      "\t\tMODEL FITTED SUCCESSFULLY.\n",
      "\tFITTING MODEL: LOGREG\n",
      "\t\tMODEL FITTED SUCCESSFULLY.\n",
      "WORKING ON DATASET: PROCESSED\n",
      "\tFITTING MODEL: KNN\n",
      "\t\tMODEL FITTED SUCCESSFULLY.\n",
      "\tFITTING MODEL: DTREE\n",
      "\t\tMODEL FITTED SUCCESSFULLY.\n",
      "\tFITTING MODEL: RFOREST\n",
      "\t\tMODEL FITTED SUCCESSFULLY.\n",
      "\tFITTING MODEL: SVC\n",
      "\t\tMODEL FITTED SUCCESSFULLY.\n",
      "\tFITTING MODEL: LOGREG\n",
      "\t\tMODEL FITTED SUCCESSFULLY.\n",
      "WORKING ON DATASET: REDUCED\n",
      "\tFITTING MODEL: KNN\n",
      "\t\tMODEL FITTED SUCCESSFULLY.\n",
      "\tFITTING MODEL: DTREE\n",
      "\t\tMODEL FITTED SUCCESSFULLY.\n",
      "\tFITTING MODEL: RFOREST\n",
      "\t\tMODEL FITTED SUCCESSFULLY.\n",
      "\tFITTING MODEL: SVC\n"
     ]
    }
   ],
   "source": [
    "for dataset in param_table.keys():\n",
    "    print(\"WORKING ON DATASET: {}\".format(dataset.upper()))\n",
    "    for model in model_hyperparams.keys():\n",
    "        print(\"\\tFITTING MODEL: {}\".format(model.upper()))\n",
    "        classifier = model_hyperparams[model][0]()\n",
    "        clf_grid = GridSearchCV(classifier, model_hyperparams[model][1], cv=5, verbose=0)\n",
    "        optimal_model = clf_grid.fit(param_table[dataset][0][0], y_train_pro)\n",
    "        param_table[dataset][1][model] = optimal_model.best_estimator_.get_params()\n",
    "        print(\"\\t\\tMODEL FITTED SUCCESSFULLY.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Section D: Deep Learning <a name=\"section-D\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Appendix A: Supplementary Custom Objects <a name=\"appendix-A\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
