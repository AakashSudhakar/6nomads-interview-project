{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Intermediate Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook, we further investigate the interim datasets through a **processing** lens: we analyze, transform, scale, encode, reduce, and otherwise munge our data to prepare it for predictive analysis and machine learning-based modeling. \n",
    "\n",
    "- **NOTE**: Before working through this notebook, please ensure that you have all necessary dependencies as denoted in [Section A: Imports and Initializations](#section-A) of this notebook.\n",
    "\n",
    "- **NOTE**: Before working through Sections A-D of this notebook, please run all code cells in [Appendix A: Supplementary Custom Objects](#appendix-A) to ensure that all relevant functions and objects are appropriately instantiated and ready for use.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîµ TABLE OF CONTENTS üîµ <a name=\"TOC\"></a>\n",
    "\n",
    "Use this **table of contents** to navigate the various sections of the processing notebook.\n",
    "\n",
    "#### 1. [Section A: Imports and Initializations](#section-A)\n",
    "\n",
    "    All necessary imports and object instantiations for data processing.\n",
    "\n",
    "#### 2. [Section B: Specialized Encoding](#section-B)\n",
    "\n",
    "    Data encoding operations, including value range mapping, \n",
    "    correlational plotting, and categorical encoding.\n",
    "\n",
    "#### 3. [Section C: Data Scaling & Transformation](#section-C)\n",
    "\n",
    "    Data transformation techniques, including standard scaling/normalization\n",
    "    and feature reduction techniques.\n",
    "\n",
    "#### 4. [Section D: Saving Our Processed Datasets](#section-D)\n",
    "\n",
    "    Saving processed data states for further access.\n",
    "\n",
    "#### 5. [Appendix A: Supplementary Custom Objects](#appendix-A)\n",
    "\n",
    "    Custom Python object architectures used throughout the data processing.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ Section A: Imports and Initializations <a name=\"section-A\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Importations for Data Manipulation and Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms for Data Scaling and Feature Reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Algorithmic Structures for Processed Data Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../structures/\")\n",
    "# from dataset_processor import Dataset_Processor\n",
    "from custom_structures import cmat_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate Our Processor Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Processor Class for Target-Oriented Data Modification.\n",
    "\n",
    "**NOTE**: Please refer to _Appendix A: Supplementary Custom Objects_ to view the fully implemented processor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = Dataset_Processor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ Section B: Data Encoding <a name=\"section-B\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Our Preprocessed Data Into Conditional DataFrame(s)\n",
    "\n",
    "**Call** `.load_data()` **method to load in all conditionally separated interim datasets.**\n",
    "\n",
    "_NOTE_: Currently loading in both datasets independently using defaulted condition `which=\"both\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_train_i, df_test_i) = proc.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚≠êÔ∏è _TODO_: Reencode each feature using binary, tertiary, and quarternary encoding schemas. ‚≠êÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ Section C: Data Scaling & Transformation <a name=\"section-C\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ Section D: Saving Our Processed Datasets <a name=\"section-D\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ Appendix A: Supplementary Custom Objects <a name=\"appendix-A\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A[1]: 6Nomads Dataset Processor.\n",
    "\n",
    "To view the **Data Processor Engine**, please follow the following steps:\n",
    "\n",
    "1. Navigate to the `structures` sibling directory. \n",
    "2. Access the `dataset_processor.py` file. \n",
    "3. View the `Dataset_Processor()` object architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Processor(object):\n",
    "    \"\"\" Class object instance for processing and transforming 6Nomads data for predictive analytics. \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializer method for object instance creation. \"\"\"\n",
    "        self.REL_PATH_TO_INT_DATA_TRAIN = \"../data/interim/train_i.csv\"\n",
    "        self.REL_PATH_TO_INT_DATA_TEST = \"../data/interim/test_i.csv\"\n",
    "        \n",
    "    def load_data(self, which=\"both\"):\n",
    "        \"\"\" \n",
    "        Instance method to load in dataset(s) into conditionally separated/joined Pandas DataFrame(s). \n",
    "        \n",
    "        INPUTS:\n",
    "            {which}:\n",
    "                - str(both): Reads in training and testing data files as tuple of individual DataFrames. (DEFAULT)\n",
    "                - str(all): Reads in training and testing data files as single conjoined DataFrame.\n",
    "                - str(train): Reads in training data file as single DataFrame.\n",
    "                - str(test): Reads in testing data file as single DataFrame.\n",
    "                \n",
    "        OUTPUTS:\n",
    "            pandas.DataFrame: Single or multiple Pandas DataFrame object(s) containing relevant data.\n",
    "        \"\"\"\n",
    "        # Validate conditional data loading arguments\n",
    "        if which not in [\"all\", \"both\", \"train\", \"test\"]:\n",
    "            raise ValueError(\"ERROR: Inappropriate value passed to argument `which`.\\n\\nExpected value in range:\\n - all\\n - both\\n - train\\n - test\\n\\nActual:\\n - {}\".format(which))\n",
    "        \n",
    "        # Independently load training data\n",
    "        if which == \"train\":\n",
    "            return pd.read_csv(self.REL_PATH_TO_INT_DATA_TRAIN, index_col=0)\n",
    "        \n",
    "        # Independently load testing data\n",
    "        if which == \"test\":\n",
    "            return pd.read_csv(self.REL_PATH_TO_INT_DATA_TEST, index_col=0)\n",
    "        else:\n",
    "            df_train = pd.read_csv(self.REL_PATH_TO_INT_DATA_TRAIN, index_col=0)\n",
    "            df_test = pd.read_csv(self.REL_PATH_TO_INT_DATA_TEST, index_col=0)\n",
    "            \n",
    "            # Load merged training and testing data\n",
    "            if which == \"all\":\n",
    "                return pd.concat([df_train, df_test], keys=[\"train\", \"test\"], sort=True)\n",
    "            \n",
    "            # Load separated training and testing data (DEFAULT)\n",
    "            if which == \"both\":\n",
    "                return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
