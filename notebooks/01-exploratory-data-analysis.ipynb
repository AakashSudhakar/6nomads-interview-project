{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook, we analyze the given external datasets through a **preprocessing** lens: we manipulate, curate, and prepare data to better understand what we're dealing with and to prepare our input data for more advanced prediction-driven modifications.\n",
    "\n",
    "- **NOTE**: Before working through this notebook, please ensure that you have all necessary dependencies as denoted in [Section A: Imports and Initializations](#section-A) of this notebook.\n",
    "\n",
    "- **NOTE**: Before working through Sections A-D of this notebook, please run all code cells in [Appendix A: Supplementary Custom Objects](#appendix-A) to ensure that all relevant functions and objects are appropriately instantiated and ready for use.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔵 TABLE OF CONTENTS 🔵 <a name=\"TOC\"></a>\n",
    "\n",
    "Use this **table of contents** to navigate the various sections of the preprocessing notebook.\n",
    "\n",
    "#### 1. [Section A: Imports and Initializations](#section-A)\n",
    "\n",
    "    All necessary imports and object instantiations for data preprocessing.\n",
    "    \n",
    "#### 2. [Section B: Manipulating Our Datasets](#section-B)\n",
    "\n",
    "    Data manipulation operations, including null value removal/imputation, \n",
    "    data splitting/merging, and data frequency generation.\n",
    "\n",
    "#### 3. [Section C: Visualizing Trends Across Our Data](#section-C)\n",
    "\n",
    "    Data visualizations to outline trends and patterns inherent across our data\n",
    "    that may mandate further analysis.\n",
    "\n",
    "#### 4. [Section D: Saving Our Interim Datasets](#section-D)\n",
    "\n",
    "    Saving preprocessed data states for further access.\n",
    "\n",
    "#### 5. [Appendix A: Supplementary Custom Objects](#appendix-A)\n",
    "\n",
    "    Custom Python object architectures used throughout the data preprocessing.\n",
    "\n",
    "#### 6. [Appendix B: Data Dictionary](#appendix-B)\n",
    "\n",
    "    Data dictionary representation for our dataset.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 Section A: Imports and Initializations <a name=\"section-A\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Importations for Data Manipulation and Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Algorithmic Structures for Data Preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../structures/\")\n",
    "from custom_structures import corrplot_\n",
    "# from dataset_preprocessor import Dataset_Preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate our Preprocessor Engine\n",
    "\n",
    "Custom Preprocessor Class for Directed Data Manipulation.\n",
    "\n",
    "**NOTE**: Please refer to _Appendix A: Supplementary Custom Objects_ for instructions on how to view the fully implemented dataset preprocessor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = Dataset_Preprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 Section B: Manipulating Our Datasets <a name=\"section-B\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Our Raw Data Into Conditional DataFrame(s)\n",
    "\n",
    "**Call** `.load_data()` **method to load in all conditionally separated external datasets.**\n",
    "\n",
    "_NOTE_: Currently loading in both datasets independently using defaulted condition `which=\"both\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_train, df_test) = preproc.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Unique Values Across Each Feature in Training Dataset.\n",
    "\n",
    "**Call the** `get_uniques()` **custom function to identify unique values across all input features for dataset(s).**\n",
    "\n",
    "_NOTE_: Currently identifying unique data values across all features in dataset using defaulted conditions `features=None` and `how=\"value\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques_train, uniques_test = get_uniques(df_train), get_uniques(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Which Features Across Training/Testing Data Contain `NaN` (Null) Values.\n",
    "\n",
    "_NOTE_: Null values are denoted as `np.nan` (float) datatypes and will appear as `<type 'float'>` data notations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "floating_features_train = identify_typed_features(uniques_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _RESULT_: No null values detected across any features in training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDENTIFIED FEATURE OF TYPE '<type 'float'>': -28\n",
      "IDENTIFIED FEATURE OF TYPE '<type 'float'>': 41\n",
      "IDENTIFIED FEATURE OF TYPE '<type 'float'>': -11\n",
      "IDENTIFIED FEATURE OF TYPE '<type 'float'>': -10\n",
      "IDENTIFIED FEATURE OF TYPE '<type 'float'>': -19\n",
      "IDENTIFIED FEATURE OF TYPE '<type 'float'>': -5.1\n",
      "IDENTIFIED FEATURE OF TYPE '<type 'float'>': -33\n",
      "IDENTIFIED FEATURE OF TYPE '<type 'float'>': -3\n",
      "IDENTIFIED FEATURE OF TYPE '<type 'float'>': -2\n"
     ]
    }
   ],
   "source": [
    "floating_features_test = identify_typed_features(uniques_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _RESULT_: Null values potentially detected across nine (9) features in testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm Null Value Presence Across Identified \"Floating\" Features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-28     True\n",
       "41      True\n",
       "-11     True\n",
       "-10     True\n",
       "-19     True\n",
       "-5.1    True\n",
       "-33     True\n",
       "-3      True\n",
       "-2      True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_null_metrics(df_test, \n",
    "                 subset=floating_features_test, \n",
    "                 metric=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Proportion of Null Values Across Each Identified \"Floating\" Feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-28     0.000667\n",
       "41      0.000333\n",
       "-11     0.000333\n",
       "-10     0.000333\n",
       "-19     0.000333\n",
       "-5.1    0.001000\n",
       "-33     0.000333\n",
       "-3      0.000333\n",
       "-2      0.000333\n",
       "dtype: float64"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_null_metrics(df_test, \n",
    "                 subset=floating_features_test, \n",
    "                 metric=\"percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute Null Values Across Floating Features\n",
    "\n",
    "**NOTE**: Since null values are highly sparse across our data (highest frequent occurrency is ~0.1%) and the size of our data is not small, we can safely drop null values rather than performing advanced imputation (e.g. _null value flagging_, _mean/mode replacement_). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null imputation has successfully completed.\n"
     ]
    }
   ],
   "source": [
    "preproc.null_imputer(df_test, \n",
    "                     subset=floating_features_test, \n",
    "                     method=\"drop\", \n",
    "                     na_filter=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reencode Alphabetical Features for Numerical Encoding Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOOKUP_TABLE_ALPHANUMERIC = {\n",
    "    1: \"A\", \n",
    "    2: \"B\", \n",
    "    3: \"C\", \n",
    "    4: \"D\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NOTE_: Feature encoding occurs inplace; if condition `drop_og` is `True`, then rerunning method call will result in errors due to dropped target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.feature_encoder(df_train,\n",
    "                        target=\"C\",\n",
    "                        lookup_table=LOOKUP_TABLE_ALPHANUMERIC,\n",
    "                        drop_og=True)\n",
    "\n",
    "preproc.feature_encoder(df_test,\n",
    "                        target=\"A\",\n",
    "                        lookup_table=LOOKUP_TABLE_ALPHANUMERIC,\n",
    "                        drop_og=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ⭐️ CURRENT TASK: Finalize Notebook and Jump to Processing! ⭐️\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🔸 CHECKPOINT 🔸\n",
    "\n",
    "**Interim data ready to save.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 Section C: Visualizing Trends Across Our Data <a name=\"section-C\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 Section D: Saving Our Interim Datasets <a name=\"section-D\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interim datasets are data states directly after preprocessing, where data is designated for curation and manipulation prior to target vs. non-target handling.\n",
    "\n",
    "#### Save Current (Preprocessed) Data States to Interim Datasets\n",
    "\n",
    "**Call** `.save_dataset()` **method to save data state to interim folder for processing accessability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "REL_PATH_TO_ITM_DATA = \"../data/interim/\"\n",
    "FILENAME_TRAINING, FILENAME_TESTING = \"train_i\", \"test_i\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.save_dataset(df_train, REL_PATH_TO_ITM_DATA + FILENAME_TRAINING)\n",
    "preproc.save_dataset(df_test, REL_PATH_TO_ITM_DATA + FILENAME_TESTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 Appendix A: Supplementary Custom Objects <a name=\"appendix-A\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A[1]: 6Nomads Dataset Preprocessor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the **Data Preprocessor Engine**, please follow the following steps:\n",
    "\n",
    "1. Navigate to the `structures` sibling directory. \n",
    "2. Access the `dataset_preprocessor.py` file. \n",
    "3. View the `Dataset_Preprocessor()` object architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NOTE_: **Creating Preprocessor Engine in Notebook Until Further Separation of Concerns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Preprocessor(object):\n",
    "    \"\"\" Class object instance for preprocessing and cleaning 6Nomads data for predictive analytics. \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializer method for object instance creation. \"\"\"\n",
    "        self.REL_PATH_TO_EXT_DATA_TRAIN = \"../data/external/train.csv\"\n",
    "        self.REL_PATH_TO_EXT_DATA_TEST = \"../data/external/test.csv\"\n",
    "        \n",
    "    def load_data(self, which=\"both\"):\n",
    "        \"\"\" \n",
    "        Instance method to load in dataset(s) into conditionally separated/joined Pandas DataFrame(s). \n",
    "        \n",
    "        INPUTS:\n",
    "            {which}:\n",
    "                - str(both): Reads in training and testing data files as tuple of individual DataFrames. (DEFAULT)\n",
    "                - str(all): Reads in training and testing data files as single conjoined DataFrame.\n",
    "                - str(train): Reads in training data file as single DataFrame.\n",
    "                - str(test): Reads in testing data file as single DataFrame.\n",
    "                \n",
    "        OUTPUTS:\n",
    "            pandas.DataFrame: Single or multiple Pandas DataFrame object(s) containing relevant data.\n",
    "        \"\"\"\n",
    "        # Validate conditional data loading arguments\n",
    "        if which not in [\"all\", \"both\", \"train\", \"test\"]:\n",
    "            raise ValueError(\"ERROR: Inappropriate value passed to argument `which`.\\n\\nExpected value in range:\\n - all\\n - both\\n - train\\n - test\\n\\nActual:\\n - {}\".format(which))\n",
    "        \n",
    "        # Independently load training data\n",
    "        if which == \"train\":\n",
    "            return pd.read_csv(self.REL_PATH_TO_EXT_DATA_TRAIN)\n",
    "        \n",
    "        # Independently load testing data\n",
    "        if which == \"test\":\n",
    "            return pd.read_csv(self.REL_PATH_TO_EXT_DATA_TEST)\n",
    "        else:\n",
    "            df_train = pd.read_csv(self.REL_PATH_TO_EXT_DATA_TRAIN)\n",
    "            df_test = pd.read_csv(self.REL_PATH_TO_EXT_DATA_TEST)\n",
    "            \n",
    "            # Load merged training and testing data\n",
    "            if which == \"all\":\n",
    "                return pd.concat([df_train, df_test], keys=[\"train\", \"test\"], sort=True)\n",
    "            \n",
    "            # Load separated training and testing data (DEFAULT)\n",
    "            if which == \"both\":\n",
    "                return df_train, df_test\n",
    "            \n",
    "    def null_imputer(self, datasets, subset=None, method=\"fill\", na_replace=-1.0, na_filter=\"all\"):\n",
    "        \"\"\"\n",
    "        Instance method to modify (replace or remove) occurrent null (`np.nan`) values across an entire dataset.\n",
    "        \n",
    "        INPUTS:\n",
    "            {datasets}:\n",
    "                - list: List of datasets; used for iterative data manipulation.\n",
    "                - pd.DataFrame: Single dataset; converted into list for general data manipulation.\n",
    "            {subset}:\n",
    "                - NoneType: If no argument passed, null value imputation occurs for all features in data. (DEFAULT: None)\n",
    "                - list: Array of feature names to iterate through for null value modification.\n",
    "            {method}:\n",
    "                - str(fill): Modifies data entries containing null value occurrences. \n",
    "                - str(drop): Removes data entries containing null value occurrences. \n",
    "            {na_replace}:\n",
    "                - NoneType(None): Removes all occurrences of null values across dataset(s).\n",
    "                - int: Replaces all occurrences of null values with given integer argument.\n",
    "                - float: Replaces all occurrences of null values with given float argument. (DEFAULT: -1.0)\n",
    "                - str(mode): Replaces all occurrences of null values with most common values in current sample.\n",
    "                - str(mean): Replaces all occurrences of null values with average of values in current sample.\n",
    "                - str(knn): Replaces all occurrences of null values with kNN-approximated data in current sample. (NOT IMPLEMENTED)\n",
    "            {na_filter}:\n",
    "                - str(any): Drops data sample if any feature contains null values.\n",
    "                - str(all): Drops data sample only if all features contain null values.\n",
    "                \n",
    "        OUTPUTS:\n",
    "            NoneType: Null value modification is performed inplace and does not return new object(s).\n",
    "        \"\"\"\n",
    "        # Validation for method instantiation with single passed DataFrame on `datasets` argument.\n",
    "        if type(datasets) is not list:\n",
    "            datasets = [datasets]\n",
    "            \n",
    "        # Feature-wise inplace null value modification using given replacement data.\n",
    "        if method == \"fill\":\n",
    "            for dataset in datasets:\n",
    "                if not subset:\n",
    "                    if na_replace == \"mode\":\n",
    "                        dataset.T.fillna(dataset.mode(axis=1)[0], inplace=True)\n",
    "                    if na_replace == \"mean\":\n",
    "                        dataset.T.fillna(dataset.mean(axis=1), inplace=True)\n",
    "                    else:\n",
    "                        for feature in dataset:\n",
    "                            dataset[feature].fillna(na_replace, inplace=True)\n",
    "                else:\n",
    "                    if na_replace == \"mode\":\n",
    "                        # NOTE: Modifies all null values with mode of given subset of data\n",
    "                        dataset.T.fillna(dataset[subset].mode(axis=1)[0], inplace=True)\n",
    "                    if na_replace == \"mean\":\n",
    "                        # NOTE: Modifies all null values with mean of given subset of data\n",
    "                        dataset.T.fillna(dataset[subset].mean(axis=1), inplace=True)\n",
    "                    else:\n",
    "                        for feature in subset:\n",
    "                            if feature in dataset:\n",
    "                                dataset[feature].fillna(na_replace, inplace=True)\n",
    "        elif method == \"drop\":\n",
    "            for dataset in datasets:\n",
    "                if not subset:\n",
    "                    for feature in dataset:\n",
    "                        dataset[feature].dropna(how=na_filter, inplace=True)\n",
    "                else:\n",
    "                    dataset.dropna(how=na_filter, subset=subset, inplace=True)\n",
    "        print(\"Null imputation has successfully completed.\")\n",
    "        \n",
    "    def feature_encoder(self, datasets, target, lookup_table, dtype=\"discrete\", drop_og=False):\n",
    "        \"\"\" \n",
    "        Instance method to iteratively encode labels in dataset as numerically categorical data.\n",
    "        \n",
    "        INPUTS:\n",
    "            {datasets}:\n",
    "                - pd.DataFrame: Single dataset; cast to list for iterative feature mapping.\n",
    "                - list: List of datasets; used for iterative feature mapping.\n",
    "            {target}:\n",
    "                - str: Name of target feature in dataset containing labels on which to encode. \n",
    "            {lookup_table}:\n",
    "                - dict: Encoding table with unencoded data ranges as values and encoded numerical categories as keys.\n",
    "            {dtype}:\n",
    "                - str(discrete): Data type parameter; indicates presence of discretized values across dataset. (DEFAULT)\n",
    "                - str(continuous): Data type parameter; indicates presence of continuous values across dataset.\n",
    "            {drop_og}:\n",
    "                - bool(True): Dataset drops original feature after encoding.\n",
    "                - bool(False): Dataset does not drop original feature after encoding. (DEFAULT)\n",
    "            \n",
    "        OUTPUTS:\n",
    "            NoneType: Dataset insertion is performed inplace and does not return new object(s).\n",
    "        \"\"\"\n",
    "        if type(datasets) is not list:\n",
    "            datasets = [datasets]\n",
    "            \n",
    "        def _encoder_helper(label, lookup_table, dtype):\n",
    "            \"\"\"\n",
    "            Custom helper function to replace unencoded label with encoded value from custom lookup table.\n",
    "\n",
    "            INPUTS:\n",
    "                {label}:\n",
    "                    - int: Unencoded value within Pandas Series to alter to categorically encoded label.\n",
    "                {lookup_table}:\n",
    "                    - dict: Encoding table with unencoded data ranges as values and encoded numerical categories as keys.\n",
    "                {dtype}:\n",
    "                    - str(discrete): Data type parameter; indicates presence of discretized labels.\n",
    "                    - str(continuous): Data type parameter; indicates presence of continuous labels.\n",
    "\n",
    "            OUTPUTS:\n",
    "                int: Encoded numerical category as new label. (DEFAULT)\n",
    "                str: Encoded string-based category as new label.\n",
    "            \"\"\"\n",
    "            for key, value in lookup_table.items():\n",
    "                if dtype == \"discrete\":\n",
    "                    if label in value:\n",
    "                        return key\n",
    "                if dtype == \"continuous\":\n",
    "                    if value[0] <= label < value[1]:\n",
    "                        return key\n",
    "        \n",
    "        encoded_feature = \"{}_encoded\".format(target)\n",
    "        for dataset in datasets:\n",
    "            if encoded_feature in dataset:\n",
    "                dataset.drop(columns=[encoded_feature], inplace=True)\n",
    "                \n",
    "            features = dataset.columns.tolist()\n",
    "            dataset.insert(loc=features.index(target) + 1, \n",
    "                           column=\"{}_encoded\".format(target), \n",
    "                           value=dataset[target].apply(_encoder_helper, \n",
    "                                                       lookup_table=lookup_table,\n",
    "                                                       dtype=dtype))\n",
    "            if drop_og:\n",
    "                dataset.drop(columns=[target], inplace=True)\n",
    "        return\n",
    "    \n",
    "    def save_dataset(self, dataset, savepath, filetype=\"csv\"):\n",
    "        \"\"\"\n",
    "        Instance method to save current state of dataset to data file accessible by navigating the parent directory.\n",
    "        \n",
    "        INPUTS:\n",
    "            {dataset}:\n",
    "                - pd.DataFrame: Single parent dataset; used for data formatting and allocation (save to memory).\n",
    "            {savepath}:\n",
    "                - str: Relative path location within parent directory to which dataset is saved.\n",
    "            {filetype}:\n",
    "                - str(csv): Data formatting type as which dataset is saved. (DEFAULT)\n",
    "                \n",
    "        OUTPUTS:\n",
    "            NoneType: Saving data to memory is performed outside the context of the object and does not return new object(s).\n",
    "        \"\"\"\n",
    "        # Sanitization for method instantiation if unknown value is passed to `filetype` keyword argument\n",
    "        if filetype not in [\"csv\", \"excel\"]:\n",
    "            raise ValueError(\"Value passed to keyword argument `filetype` is uninterpretable. EXPECTED: ['csv', 'excel']. ACTUAL: ['{}']\".format(filetype))\n",
    "        \n",
    "        # Explicit relative pathway declaration and saving process performed on dataset\n",
    "        savepath += \".{}\".format(filetype)\n",
    "        if filetype == \"csv\":\n",
    "            dataset.to_csv(savepath)\n",
    "        elif filetype == \"excel\":\n",
    "            dataset.to_excel(savepath)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A[2]: Function to Obtain Relevant Unique Values or Data Types from Feature(s) Across Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniques(dataset, features=None, how=\"both\"):\n",
    "    \"\"\"\n",
    "    Custom function that analyzes a dataset's given feature(s) and returns all unique values or data types\n",
    "    across each inputted feature.\n",
    "    \n",
    "    INPUTS:\n",
    "        {features}:\n",
    "            - NoneType(None): Sets function to use all features across dataset. (DEFAULT)\n",
    "            - str: Single referenced feature in dataset.\n",
    "            - list: List of referenced features in dataset.\n",
    "        {how}:\n",
    "            - str(both): Identifies both unique data types and values. (DEFAULT)\n",
    "            - str(dtype): Identifies unique data types.\n",
    "            - str(value): Identifies unique data values.\n",
    "    \n",
    "    OUTPUTS:\n",
    "        dict(uniques): Dictionary structure mapping each input feature to relevantly identified unique values/types.\n",
    "    \"\"\"\n",
    "    # Validate selected features argument\n",
    "    if features is not None and type(features) not in [str, list]:\n",
    "        raise TypeError(\"ERROR: Inappropriate data type passed to argument `features`.\\n\\nExpected type in range:\\n - NoneType\\n - str()\\n - list()\\n\\nActual:\\n - {}\".format(str(type(features))))\n",
    "    \n",
    "    # Validate unique identifier argument\n",
    "    if how not in [\"both\", \"dtype\", \"value\"]:\n",
    "        raise ValueError(\"ERROR: Inappropriate value passed to argument `how`.\\n\\nExpected value in range:\\n - both\\n - dtype\\n - value\\n\\nActual:\\n - {}\".format(how))\n",
    "        \n",
    "    # Reformat `features` object into list\n",
    "    if features is None:\n",
    "        features = dataset.columns.tolist()\n",
    "    if type(features) == str:\n",
    "        features = [features]\n",
    "        \n",
    "    # Create uniques object and iteratively map each feature to associated unique data\n",
    "    uniques = dict()\n",
    "    # Create dictionary object associating feature(s) and unique data types and values\n",
    "    if how == \"both\":\n",
    "        unique_types, unique_values = dict(), dict()\n",
    "        for feature in features:\n",
    "            unique_types[feature] = list(set(map(type, dataset[feature])))\n",
    "            unique_values[feature] = sorted(dataset[feature].unique().tolist())\n",
    "        unique_components = [unique_types, unique_values]\n",
    "        for feature in unique_types.keys():\n",
    "            uniques[feature] = {\"dtypes\": unique_components[0][feature], \"values\": unique_components[1][feature]}\n",
    "    else:\n",
    "        for feature in features:\n",
    "            # Create dictionary object associating feature(s) and unique data types\n",
    "            if how == \"dtype\":\n",
    "                uniques[feature] = list(set(map(type, dataset[feature])))\n",
    "            # Create dictionary object associating feature(s) and unique values\n",
    "            if how == \"value\":\n",
    "                uniques[feature] = sorted(dataset[feature].unique().tolist())\n",
    "    return uniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A[3]: Function to Identify and Return Features Containing Unique Input Data Types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_typed_features(uniques, dtype=float):\n",
    "    \"\"\"\n",
    "    Custom function that extracts features from previously generated unique feature data\n",
    "    based on whether or not feature includes user-specified data type.\n",
    "    \n",
    "    INPUTS: \n",
    "        {uniques}:\n",
    "            - dict: Dictionary object of feature associations generated by `get_uniques()`.\n",
    "        {dtype}:\n",
    "            - type(float): Float data type. (DEFAULT)\n",
    "            - type(int): Integer data type.\n",
    "            - type(str): String data type.\n",
    "    \n",
    "    OUTPUTS:\n",
    "        list(typed_features): List of feature names corresponding to identified user-specified data types.\n",
    "    \"\"\"\n",
    "    typed_features = list()\n",
    "    for key in uniques.keys():\n",
    "        if uniques[key][\"dtypes\"][0] == dtype:\n",
    "            print(\"IDENTIFIED FEATURE OF TYPE '{}': {}\".format(str(dtype), key))\n",
    "            typed_features.append(key)\n",
    "    return typed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A[4]: Function to Calculate Null/Missing Metrics of Given Feature Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_null_metrics(dataset, subset=None, metric=\"percent\"):\n",
    "    \"\"\"\n",
    "    Custom function that produces series of associated features and metrics related to\n",
    "    presence and proportion of null/missing values.\n",
    "    \n",
    "    INPUTS:\n",
    "        {dataset}:\n",
    "            - pd.DataFrame: Single input dataset.\n",
    "        {subset}:\n",
    "            - NoneType: If None, all features are used for null metric evaluation. (DEFAULT)\n",
    "            - list: Array of features across data to consider; others are ignored.\n",
    "        {metric}:\n",
    "            - str(percent): Determines calculation of relative proportions of null values per feature. (DEFAULT)\n",
    "            - str(count): Determines calculation of absolute count of null values per feature.\n",
    "            - str(binary): Determines identification of whether or not any null values occur per feature.\n",
    "    \n",
    "    OUTPUTS:\n",
    "        pd.Series: Series of associated feature names and relative null value prevalence metrics.\n",
    "    \"\"\"\n",
    "    # Validate `subset` keyword argument\n",
    "    if subset is None:\n",
    "        subset = dataset.columns.tolist()\n",
    "        \n",
    "    # Calculate percentages for null values across each input feature\n",
    "    if metric == \"percent\":\n",
    "        return dataset[subset].isna().sum() / len(dataset)\n",
    "    # Calculate total counts of null values across each input feature\n",
    "    elif metric == \"count\":\n",
    "        return dataset[subset].isna().sum()\n",
    "    # Determine True/False based on null value presence across each input feature\n",
    "    elif metric == \"binary\":\n",
    "        binarized_metrics = list()\n",
    "        for feature in subset:\n",
    "            nulls_in_feature = dataset[feature].isna().values.any()\n",
    "            binarized_metrics.append((feature, nulls_in_feature))\n",
    "        binarized_metrics_series = list(zip(*binarized_metrics))\n",
    "        return pd.Series(binarized_metrics_series[1], index=binarized_metrics_series[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 Appendix B: Data Dictionary <a name=\"appendix-B\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
