{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook, we analyze the given external datasets through a **preprocessing** lens: we manipulate, curate, and prepare data to better understand what we're dealing with and to prepare our input data for more advanced prediction-driven modifications.\n",
    "\n",
    "- **NOTE**: Before working through this notebook, please ensure that you have all necessary dependencies as denoted in [Section A: Imports and Initializations](#section-A) of this notebook.\n",
    "\n",
    "- **NOTE**: Before working through Sections A-D of this notebook, please run all code cells in [Appendix A: Supplementary Custom Objects](#appendix-A) to ensure that all relevant functions and objects are appropriately instantiated and ready for use.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”µ TABLE OF CONTENTS ðŸ”µ <a name=\"TOC\"></a>\n",
    "\n",
    "Use this **table of contents** to navigate the various sections of the preprocessing notebook.\n",
    "\n",
    "#### 1. [Section A: Imports and Initializations](#section-A)\n",
    "\n",
    "    All necessary imports and object instantiations for data preprocessing.\n",
    "    \n",
    "#### 2. [Section B: Manipulating Our Datasets](#section-B)\n",
    "\n",
    "    Data manipulation operations, including null value removal/imputation, \n",
    "    data splitting/merging, and data frequency generation.\n",
    "\n",
    "#### 3. [Section C: Visualizing Trends Across Our Data](#section-C)\n",
    "\n",
    "    Data visualizations to outline trends and patterns inherent across our data\n",
    "    that may mandate further analysis.\n",
    "\n",
    "#### 4. [Section D: Saving Our Interim Datasets](#section-D)\n",
    "\n",
    "    Saving preprocessed data states for further access.\n",
    "\n",
    "#### 5. [Appendix A: Supplementary Custom Objects](#appendix-A)\n",
    "\n",
    "    Custom Python object architectures used throughout the data preprocessing.\n",
    "\n",
    "#### 6. [Appendix B: Data Dictionary](#appendix-B)\n",
    "\n",
    "    Data dictionary representation for our dataset.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Section A: Imports and Initializations <a name=\"section-A\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Importations for Data Manipulation and Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Algorithmic Structures for Data Preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../structures/\")\n",
    "# from dataset_preprocessor import Dataset_Preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate our Preprocessor Engine\n",
    "\n",
    "Custom Preprocessor Class for Directed Data Manipulation.\n",
    "\n",
    "**NOTE**: Please refer to _Appendix A: Supplementary Custom Objects_ for instructions on how to view the fully implemented dataset preprocessor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = Dataset_Preprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Our Raw Data Into Conditional DataFrame(s)\n",
    "\n",
    "**Call** `.load_data()` **method to load in all conditionally separated external datasets.**\n",
    "\n",
    "_NOTE_: Currently loading in both datasets independently using defaulted condition `which=\"both\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_train, df_test) = preproc.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Unique Values Across Each Feature in Training Dataset.\n",
    "\n",
    "**Call the** `get_uniques()` **custom function to identify unique values across all input features for dataset(s).**\n",
    "\n",
    "_NOTE_: Currently identifying unique data values across all features in dataset using defaulted conditions `features=None` and `how=\"value\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_train, unique_values_test = get_uniques(df_train), get_uniques(df_test)\n",
    "unique_types_train, unique_types_test = get_uniques(df_train, how=\"dtype\"), get_uniques(df_test, how=\"dtype\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ”¸ NEXT TASK: _Merge both dictionaries by key into single data dictionary._ ðŸ”¸\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-1': [int],\n",
       " '-10': [float],\n",
       " '-10.1': [int],\n",
       " '-10.2': [int],\n",
       " '-11': [float],\n",
       " '-11.1': [int],\n",
       " '-12': [int],\n",
       " '-13': [int],\n",
       " '-14': [int],\n",
       " '-14.1': [int],\n",
       " '-19': [float],\n",
       " '-19.1': [int],\n",
       " '-2': [float],\n",
       " '-2.1': [int],\n",
       " '-2.2': [int],\n",
       " '-23': [int],\n",
       " '-23.1': [int],\n",
       " '-27': [int],\n",
       " '-27.1': [int],\n",
       " '-28': [float],\n",
       " '-3': [float],\n",
       " '-3.1': [int],\n",
       " '-3.2': [int],\n",
       " '-33': [float],\n",
       " '-36': [int],\n",
       " '-4': [int],\n",
       " '-4.1': [int],\n",
       " '-4.2': [int],\n",
       " '-4.3': [int],\n",
       " '-47': [int],\n",
       " '-5': [int],\n",
       " '-5.1': [float],\n",
       " '-59': [int],\n",
       " '-6': [int],\n",
       " '-68': [int],\n",
       " '-7': [int],\n",
       " '-9': [int],\n",
       " '0': [int],\n",
       " '0.1': [int],\n",
       " '0.2': [int],\n",
       " '0.3': [int],\n",
       " '0.4': [int],\n",
       " '1': [int],\n",
       " '11': [int],\n",
       " '12': [int],\n",
       " '12.1': [int],\n",
       " '14': [int],\n",
       " '15': [int],\n",
       " '16': [int],\n",
       " '16.1': [int],\n",
       " '19': [int],\n",
       " '2': [int],\n",
       " '2.1': [int],\n",
       " '20': [int],\n",
       " '29': [int],\n",
       " '3': [int],\n",
       " '37': [int],\n",
       " '4': [int],\n",
       " '41': [float],\n",
       " '5': [int],\n",
       " '5.1': [int],\n",
       " '5.2': [int],\n",
       " '60': [int],\n",
       " '7': [int],\n",
       " '9': [int],\n",
       " 'A': [str]}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_types_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Section B: Manipulating Our Datasets <a name=\"section-B\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ”¸ CHECKPOINT ðŸ”¸\n",
    "\n",
    "**Interim data ready to save.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Section C: Visualizing Trends Across Our Data <a name=\"section-C\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Section D: Saving Our Interim Datasets <a name=\"section-D\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Appendix A: Supplementary Custom Objects <a name=\"appendix-A\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A[1]: 6Nomads Dataset Preprocessor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the **Data Preprocessor Engine**, please follow the following steps:\n",
    "\n",
    "1. Navigate to the `structures` sibling directory. \n",
    "2. Access the `dataset_preprocessor.py` file. \n",
    "3. View the `Dataset_Preprocessor()` object architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NOTE_: **Creating Preprocessor Engine in Notebook Until Further Separation of Concerns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Preprocessor(object):\n",
    "    \"\"\" Class object instance for preprocessing and cleaning 6Nomads data for predictive analytics. \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializer method for object instance creation. \"\"\"\n",
    "        self.REL_PATH_TO_EXT_DATA_TRAIN = \"../data/external/train.csv\"\n",
    "        self.REL_PATH_TO_EXT_DATA_TEST = \"../data/external/test.csv\"\n",
    "        \n",
    "    def load_data(self, which=\"both\"):\n",
    "        \"\"\" \n",
    "        Instance method to load in dataset(s) into conditionally separated/joined Pandas DataFrame(s). \n",
    "        \n",
    "        INPUTS:\n",
    "            {which}:\n",
    "                - str(both): Reads in training and testing data files as tuple of individual DataFrames. (DEFAULT)\n",
    "                - str(all): Reads in training and testing data files as single conjoined DataFrame.\n",
    "                - str(train): Reads in training data file as single DataFrame.\n",
    "                - str(test): Reads in testing data file as single DataFrame.\n",
    "                \n",
    "        OUTPUTS:\n",
    "            pandas.DataFrame: Single or multiple Pandas DataFrame object(s) containing relevant data.\n",
    "        \"\"\"\n",
    "        # Validate conditional data loading arguments\n",
    "        if which not in [\"all\", \"both\", \"train\", \"test\"]:\n",
    "            raise ValueError(\"ERROR: Inappropriate value passed to argument `which`.\\n\\nExpected value in range:\\n - all\\n - both\\n - train\\n - test\\n\\nActual:\\n - {}\".format(which))\n",
    "        \n",
    "        # Independently load training data\n",
    "        if which == \"train\":\n",
    "            return pd.read_csv(self.REL_PATH_TO_EXT_DATA_TRAIN)\n",
    "        \n",
    "        # Independently load testing data\n",
    "        if which == \"test\":\n",
    "            return pd.read_csv(self.REL_PATH_TO_EXT_DATA_TEST)\n",
    "        else:\n",
    "            df_train = pd.read_csv(self.REL_PATH_TO_EXT_DATA_TRAIN)\n",
    "            df_test = pd.read_csv(self.REL_PATH_TO_EXT_DATA_TEST)\n",
    "            \n",
    "            # Load merged training and testing data\n",
    "            if which == \"all\":\n",
    "                return pd.concat([df_train, df_test], keys=[\"train\", \"test\"], sort=True)\n",
    "            \n",
    "            # Load separated training and testing data (DEFAULT)\n",
    "            if which == \"both\":\n",
    "                return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A[2]: Function to Obtain Relevant Unique Values or Data Types from Feature(s) Across Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniques(dataset, features=None, how=\"value\"):\n",
    "    \"\"\"\n",
    "    Custom function that analyzes a dataset's given feature(s) and returns all unique values or data types\n",
    "    across each inputted feature.\n",
    "    \n",
    "    INPUTS:\n",
    "        {features}:\n",
    "            - NoneType(None): Sets function to use all features across dataset. (DEFAULT)\n",
    "            - str: Single referenced feature in dataset.\n",
    "            - list: List of referenced features in dataset.\n",
    "        {how}:\n",
    "            - str(value): Identifies unique data values. (DEFAULT)\n",
    "            - str(dtype): Identifies unique data types.\n",
    "    \n",
    "    OUTPUTS:\n",
    "        dict: Dictionary structure mapping each input feature to relevantly identified unique values/types.\n",
    "    \"\"\"\n",
    "    # Validate selected features argument\n",
    "    if features is not None and type(features) not in [str, list]:\n",
    "        raise TypeError(\"ERROR: Inappropriate data type passed to argument `features`.\\n\\nExpected type in range:\\n - NoneType\\n - str()\\n - list()\\n\\nActual:\\n - {}\".format(str(type(features))))\n",
    "    \n",
    "    # Validate unique identifier argument\n",
    "    if how not in [\"value\", \"dtype\"]:\n",
    "        raise ValueError(\"ERROR: Inappropriate value passed to argument `how`.\\n\\nExpected value in range:\\n - value\\n - dtype\\n\\nActual:\\n - {}\".format(how))\n",
    "        \n",
    "    # Reformat `features` object into list\n",
    "    if features is None:\n",
    "        features = dataset.columns.tolist()\n",
    "    if type(features) == str:\n",
    "        features = [features]\n",
    "    \n",
    "    # Create uniques object and iteratively map each feature to associated unique data\n",
    "    uniques = dict()\n",
    "    for feature in features:\n",
    "        # Create dictionary object associating feature(s) and unique values\n",
    "        if how == \"value\":\n",
    "            uniques[feature] = sorted(dataset[feature].unique().tolist())\n",
    "        # Create dictionary object associating feature(s) and unique data types\n",
    "        if how == \"dtype\":\n",
    "            uniques[feature] = list(set(map(type, dataset[feature])))\n",
    "    return uniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Appendix B: Data Dictionary <a name=\"appendix-B\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [(back to top)](#TOC)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
